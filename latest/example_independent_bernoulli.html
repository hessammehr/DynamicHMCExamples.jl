<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Estimate Bernoulli draws probabilility ¬∑ DynamicHMCExamples.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>DynamicHMCExamples.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">Overview</a></li><li class="current"><a class="toctext" href="example_independent_bernoulli.html">Estimate Bernoulli draws probabilility</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href="example_independent_bernoulli.html">Estimate Bernoulli draws probabilility</a></li></ul><a class="edit-page" href="https://github.com/tpapp/DynamicHMCExamples.jl/blob/master/src/example_independent_bernoulli.jl"><span class="fa">ÔÇõ</span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Estimate Bernoulli draws probabilility</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Estimate-Bernoulli-draws-probabilility-1" href="#Estimate-Bernoulli-draws-probabilility-1">Estimate Bernoulli draws probabilility</a></h1><p>We estimate a simple model of <span>$n$</span> independent Bernoulli draws, with probability <span>$Œ±$</span>. First, we load the packages we use.</p><div><pre><code class="language-julia">using TransformVariables
using LogDensityProblems
using DynamicHMC
using MCMCDiagnostics
using Parameters
using Statistics</code></pre></div><p>Then define a structure to hold the data. For this model, the number of draws equal to <code>1</code> is a sufficient statistic.</p><div><pre><code class="language-julia">&quot;&quot;&quot;
Toy problem using a Bernoulli distribution.

We model `n` independent draws from a ``Bernoulli(Œ±)`` distribution.
&quot;&quot;&quot;
struct BernoulliProblem
    &quot;Total number of draws in the data.&quot;
    n::Int
    &quot;Number of draws `==1` in the data&quot;
    s::Int
end</code></pre><pre><code class="language-none">Main.ex-example_independent_bernoulli.BernoulliProblem</code></pre></div><p>Then make the type callable with the parameters <em>as a single argument</em>.  We use decomposition in the arguments, but it could be done inside the function, too.</p><div><pre><code class="language-julia">function (problem::BernoulliProblem)((Œ±, )::NamedTuple{(:Œ±, )})
    @unpack n, s = problem        # extract the data
    s * log(Œ±) + (n-s) * log(1-Œ±) # log likelihood
end</code></pre></div><p>We should test this, also, this would be a good place to benchmark and optimize more complicated problems.</p><div><pre><code class="language-julia">p = BernoulliProblem(20, 10)
p((Œ± = 0.5, ))</code></pre><pre><code class="language-none">-13.862943611198906</code></pre></div><p>Recall that we need to</p><ol><li><p>transform from <span>$‚Ñù$</span> to the valid parameter domain <code>(0,1)</code> for more efficient sampling, and</p></li><li><p>calculate the derivatives for this transformed mapping.</p></li></ol><p>The helper packages <code>TransformVariables</code> and <code>LogDensityProblems</code> take care of this. We use a flat prior (the default, omitted)</p><div><pre><code class="language-julia">P = TransformedBayesianProblem(to_tuple((Œ± = to_ùïÄ,)), p);
‚àáP = ForwardDiffLogDensity(P);</code></pre></div><p>Finally, we sample from the posterior. <code>chain</code> holds the chain (positions and diagnostic information), while the second returned value is the tuned sampler which would allow continuation of sampling.</p><div><pre><code class="language-julia">chain, NUTS_tuned = NUTS_init_tune_mcmc(‚àáP, 1000)</code></pre><pre><code class="language-none">MCMC, adapting œµ (75 steps)
 ...done
MCMC, adapting œµ (25 steps)
 ...done
MCMC, adapting œµ (50 steps)
 ...done
MCMC, adapting œµ (100 steps)
step 100/100, 8.1 s/step
 ...done
MCMC, adapting œµ (200 steps)
step 100/200, 8.1 s/step
step 200/200, 6.5 s/step
 ...done
MCMC, adapting œµ (400 steps)
step 100/400, 8.5 s/step
step 200/400, 6.5 s/step
step 300/400, 7.2 s/step
step 400/400, 7.0 s/step
 ...done
MCMC, adapting œµ (50 steps)
 ...done
MCMC (1000 steps)
step 100/1000, 7.2 s/step
step 200/1000, 7.2 s/step
step 300/1000, 6.5 s/step
step 400/1000, 6.8 s/step
step 500/1000, 6.7 s/step
step 600/1000, 7.2 s/step
step 700/1000, 6.8 s/step
step 800/1000, 6.7 s/step
step 900/1000, 6.4 s/step
step 1000/1000, 6.8 s/step
 ...done
(NUTS_Transition{Array{Float64,1},Float64}[NUTS_Transition{Array{Float64,1},Float64}([0.211922], -15.5874, 1, DoubledTurn, 1.0, 1), NUTS_Transition{Array{Float64,1},Float64}([0.178388], -15.3799, 1, DoubledTurn, 1.0, 1), NUTS_Transition{Array{Float64,1},Float64}([0.0499903], -15.3167, 1, AdjacentTurn, 1.0, 3), NUTS_Transition{Array{Float64,1},Float64}([-0.0358291], -15.2636, 1, AdjacentTurn, 0.998931, 3), NUTS_Transition{Array{Float64,1},Float64}([-0.225635], -15.3996, 1, AdjacentTurn, 0.968009, 3), NUTS_Transition{Array{Float64,1},Float64}([-0.122101], -15.3629, 1, DoubledTurn, 1.0, 1), NUTS_Transition{Array{Float64,1},Float64}([0.187963], -15.667, 2, DoubledTurn, 0.93865, 3), NUTS_Transition{Array{Float64,1},Float64}([0.539198], -16.7141, 2, DoubledTurn, 0.818367, 3), NUTS_Transition{Array{Float64,1},Float64}([-0.134259], -16.192, 1, AdjacentTurn, 0.969491, 3), NUTS_Transition{Array{Float64,1},Float64}([-0.0542094], -15.343, 2, DoubledTurn, 0.993534, 3)  ‚Ä¶  NUTS_Transition{Array{Float64,1},Float64}([0.284377], -16.7682, 2, DoubledTurn, 0.791464, 3), NUTS_Transition{Array{Float64,1},Float64}([0.293805], -15.5431, 1, DoubledTurn, 0.995977, 1), NUTS_Transition{Array{Float64,1},Float64}([-0.210916], -16.6947, 2, DoubledTurn, 0.826988, 3), NUTS_Transition{Array{Float64,1},Float64}([0.151088], -15.6053, 2, DoubledTurn, 0.969377, 3), NUTS_Transition{Array{Float64,1},Float64}([0.187134], -15.3566, 1, DoubledTurn, 0.99086, 1), NUTS_Transition{Array{Float64,1},Float64}([0.406168], -15.6998, 1, DoubledTurn, 0.908208, 1), NUTS_Transition{Array{Float64,1},Float64}([0.0979886], -15.9063, 2, DoubledTurn, 0.969737, 3), NUTS_Transition{Array{Float64,1},Float64}([0.14664], -15.3109, 1, DoubledTurn, 0.991035, 1), NUTS_Transition{Array{Float64,1},Float64}([0.313632], -15.5187, 1, DoubledTurn, 0.944094, 1), NUTS_Transition{Array{Float64,1},Float64}([0.350011], -15.6443, 1, DoubledTurn, 0.982457, 1)], NUTS sampler in 1 dimensions
  stepsize (œµ) ‚âà 1.12
  maximum depth = 5
  Gaussian kinetic energy, ‚àödiag(M‚Åª¬π): [0.4001]
)</code></pre></div><p>To get the posterior for <span>$Œ±$</span>, we need to use <code>get_position</code> and then transform</p><div><pre><code class="language-julia">posterior = transform.(Ref(‚àáP.transformation), get_position.(chain));</code></pre></div><p>Extract the parameter.</p><div><pre><code class="language-julia">posterior_Œ± = first.(posterior);</code></pre></div><p>check the mean</p><div><pre><code class="language-julia">mean(posterior_Œ±)</code></pre><pre><code class="language-none">0.4949053257471374</code></pre></div><p>check the effective sample size</p><div><pre><code class="language-julia">ess_Œ± = effective_sample_size(posterior_Œ±)</code></pre><pre><code class="language-none">265.39982762940923</code></pre></div><p>NUTS-specific statistics</p><div><pre><code class="language-julia">NUTS_statistics(chain)</code></pre><pre><code class="language-none">Hamiltonian Monte Carlo sample of length 1000
  acceptance rate mean: 0.92, min/25%/median/75%/max: 0.32 0.88 0.97 1.0 1.0
  termination: AdjacentTurn =&gt; 28% DoubledTurn =&gt; 72%
  depth: 1 =&gt; 66% 2 =&gt; 34%</code></pre></div><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p><footer><hr/><a class="previous" href="index.html"><span class="direction">Previous</span><span class="title">Overview</span></a></footer></article></body></html>
